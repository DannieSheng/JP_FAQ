{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata Replacement + Node Sentence Window\n",
    "https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/MetadataReplacementDemo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-embeddings-openai\n",
    "# %pip install llama-index-embeddings-huggingface\n",
    "# %pip install llama-index-llms-openai\n",
    "# %pip install llama-index\n",
    "# # %pip install --upgrade llama-index-core llama-index-llms-openai llama-index-embeddings-openai llama-index-embeddings-huggingface\n",
    "# %pip install llamaapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAI(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x442499090>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x15c0de050>, completion_to_prompt=<function default_completion_to_prompt at 0x15c17e200>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, model='gpt-3.5-turbo', temperature=0.1, max_tokens=None, logprobs=None, top_logprobs=0, additional_kwargs={}, max_retries=3, timeout=60.0, default_headers=None, reuse_client=True, api_key='sk-zk2e65c7b3aa4d285237b360e338e2c174066b0a70704f61', api_base='https://api.openai.com/v1', api_version='')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hudanyunsheng/miniforge3/envs/jpfaq/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI as li_OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "from llama_index.core.node_parser import SentenceSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the sentence window node parser w/ default settings\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=3,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base node parser is a sentence splitter\n",
    "text_splitter = SentenceSplitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-zk2e65c7b3aa4d285237b360e338e2c174066b0a70704f61\"\n",
    "os.environ[\"llama_API_KEY\"] = \"LL-8UzIPeKYxLIbdX9x66MQ7tKWb16lOA5FWWkViLhD54piGPO3WpLkRlikijI6DNEU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'llama_index.llms.openai.base.OpenAI'>\n",
      "<class 'llamaapi.llamaapi.LlamaAPI'>\n",
      "<class 'llama_index.llms.openai.base.OpenAI'>\n"
     ]
    }
   ],
   "source": [
    "llm_openAI = li_OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "print(type(llm_openAI))\n",
    "\n",
    "from llamaapi import LlamaAPI\n",
    "llm = LlamaAPI(os.environ.get(\"llama_API_KEY\"))\n",
    "print(type(llm))\n",
    "\n",
    "llm_client = li_OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"), api_base=\"https://api.zhizengzeng.com/v1\", model=\"gpt-3.5-turbo\", temperature=0.1\n",
    ")\n",
    "print(type(llm_client))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "openai.OpenAI"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# openAIclient = openai.OpenAI(\n",
    "#     api_key = os.environ.get(\"OPENAI_API_KEY\"),\n",
    "#     base_url = \"https://api.zhizengzeng.com/v1\"\n",
    "# )\n",
    "# type(openAIclient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (this step needs vpn)\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\", max_length=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "\n",
    "# llm = LlamaAPI(os.environ.get(\"llama_API_KEY\"))\n",
    "# llm = OpenAI(\n",
    "#     api_key = os.environ.get(\"llama_API_KEY\"),\n",
    "#     base_url = \"https://api.llama-api.com\"\n",
    "# )\n",
    "\n",
    "\n",
    "llm = llm_client\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.text_splitter = text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl https://www.ipcc.ch/report/ar6/wg2/downloads/report/IPCC_AR6_WGII_Chapter03.pdf --output IPCC_AR6_WGII_Chapter03.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"./IPCC_AR6_WGII_Chapter03.pdf\"]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "base_nodes = text_splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "sentence_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_index = VectorStoreIndex(base_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is low confidence in quantifying AMOC changes in the 20th century due to disagreement in reconstructed and simulated trends. Additionally, the relative contributions of internal variability, natural forcing, and anthropogenic forcing to AMOC change cannot be determined due to the short duration of direct observational records since the mid-2000s. Despite projections indicating a decline in AMOC over the 21st century for all scenarios, there is high confidence that an abrupt collapse is not expected before 2100.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.postprocessor import MetadataReplacementPostProcessor\n",
    "\n",
    "query_engine = sentence_index.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    # the target key defaults to `window` to match the node_parser's default\n",
    "    node_postprocessors=[\n",
    "        MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    ],\n",
    ")\n",
    "window_response = query_engine.query(\n",
    "    \"What are the concerns surrounding the AMOC?\"\n",
    ")\n",
    "print(window_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "Unknown model 'llama3.1'. Please provide a valid OpenAI model name in: gpt-4, gpt-4-32k, gpt-4-1106-preview, gpt-4-0125-preview, gpt-4-turbo-preview, gpt-4-vision-preview, gpt-4-1106-vision-preview, gpt-4-turbo-2024-04-09, gpt-4-turbo, gpt-4o, gpt-4o-2024-05-13, gpt-4o-mini, gpt-4o-mini-2024-07-18, gpt-4-0613, gpt-4-32k-0613, gpt-4-0314, gpt-4-32k-0314, gpt-3.5-turbo, gpt-3.5-turbo-16k, gpt-3.5-turbo-0125, gpt-3.5-turbo-1106, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k-0613, gpt-3.5-turbo-0301, text-davinci-003, text-davinci-002, gpt-3.5-turbo-instruct, text-ada-001, text-babbage-001, text-curie-001, ada, babbage, curie, davinci, gpt-35-turbo-16k, gpt-35-turbo, gpt-35-turbo-0125, gpt-35-turbo-1106, gpt-35-turbo-0613, gpt-35-turbo-16k-0613\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='', role='assistant', function_call=None, tool_calls=None))], created=1722901905, model=None, object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=0, prompt_tokens=0, total_tokens=0), code=0, msg='ok')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=\"sk-zk2e65c7b3aa4d285237b360e338e2c174066b0a70704f61\", \n",
    "                base_url=\"https://api.zhizengzeng.com/v1\")\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"SparkDesk\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"你是哪个公司开发的什么模型？\"}\n",
    "    ]\n",
    ")\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=\"sk-zk2e65c7b3aa4d285237b360e338e2c174066b0a70704f61\", base_url=\"https://api.zhizengzeng.com/v1/\")\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"圆周率的前10位\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9t20YzNoM6KNjZxO5O3bAGRV7W5Km', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='圆周率的前10位小数为3.1415926535。', role='assistant', function_call=None, tool_calls=None))], created=1722902786, model='gpt-3.5-turbo', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=18, prompt_tokens=25, total_tokens=43), code=0, msg='ok')\n"
     ]
    }
   ],
   "source": [
    "print(reszp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jpfaq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
